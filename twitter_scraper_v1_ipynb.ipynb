{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-scraper-v1.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnpFPoyXWKQwxJQHsIW2EY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erfan-sams/twitter-crawler/blob/main/twitter_scraper_v1_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9PmSdBcxGGM"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update \n",
        "!sudo apt-get install python3.8 \n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2\n",
        "\n",
        "#check python version\n",
        "!python --version\n",
        "\n",
        "! curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py \n",
        "! python3 get-pip.py --force-reinstall \n",
        "\n",
        "!pip install git+https://github.com/erfan-sams/snscrape.git\n",
        "\n",
        "! pip install pandas \n",
        "! pip install datetime\n",
        "! pip install pytz\n",
        "! pip install tweepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_code = \"\"\"\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "from random import random\n",
        "from datetime import date\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "\n",
        "# https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/twitter.py\n",
        "# example for arguments:\n",
        "  # start_date = date.today()\n",
        "  # end_date = date(2019, 2, 1)\n",
        "  # max_results = 5000       # number of tweets\n",
        "\n",
        "\n",
        "\n",
        "class Twitter_scraper:\n",
        "\n",
        "  def __init__(self,\n",
        "               max_results: int,\n",
        "               all_words = [],\n",
        "               exact_pharase=[],\n",
        "               any_words = [],\n",
        "               none_words = [],\n",
        "               hashtags = [],\n",
        "               mentioned_users = [],\n",
        "               from_users = [],\n",
        "               to_users = [],\n",
        "               **kwargs):\n",
        "    \n",
        "    self.number_of_user = 0\n",
        "    self.max_results = max_results\n",
        "    self.all_words = Twitter_scraper.all_of_these_words(all_words)\n",
        "    self.exact_pharase = f'\\\"{exact_pharase}\\\"' if exact_pharase else ''\n",
        "    self.any_words = Twitter_scraper.any_of_these_words(any_words)\n",
        "    self.none_words = Twitter_scraper.none_of_these_words(none_words)\n",
        "    self.these_hashtags = Twitter_scraper.any_of_these_hashtags(hashtags)\n",
        "    self.mentioned_users = Twitter_scraper.mentioning_these_users(mentioned_users)\n",
        "\n",
        "    self.query_dict = {'all_words':self.all_words, 'exact_pharase':self.exact_pharase,\n",
        "                  'any_words':self.any_words, 'none_words':self.none_words, 'these_hashtags':self.these_hashtags,\n",
        "                  'mentioned_users':self.mentioned_users}\n",
        "\n",
        "    self.query_dict['from'] = Twitter_scraper.f_or_t_users(from_users, 'from')\n",
        "    self.query_dict['to'] = Twitter_scraper.f_or_t_users(to_users, 'to')\n",
        "\n",
        "    for key, value in kwargs.items():\n",
        "        self.query_dict[key] = (f'({key}:{value})')\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def f_or_t_users(users, key):\n",
        "    if not users:\n",
        "      return ''\n",
        "    tmp_list = [f'{key}:{user}' for user in users]\n",
        "    return('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def all_of_these_words(all_words):\n",
        "    if not all_words:\n",
        "      return ''\n",
        "    return ' '.join(all_words)\n",
        "\n",
        "  @staticmethod\n",
        "  def any_of_these_words(any_words):\n",
        "    if not any_words:\n",
        "      return ''\n",
        "    return ('(' + ' OR '.join(any_words) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def none_of_these_words(none_words):\n",
        "    if not none_words:\n",
        "      return ''    \n",
        "    return ('-' + ' -'.join(none_words))\n",
        "\n",
        "  @staticmethod\n",
        "  def any_of_these_hashtags(hashtags):\n",
        "    if not hashtags:\n",
        "      return ''    \n",
        "    tmp_list = ['#'+ h.replace('#','') for h in hashtags]\n",
        "    return ('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def mentioning_these_users(users):\n",
        "    if not users:\n",
        "      return ''    \n",
        "    tmp_list = ['@'+ h.replace('@','') for h in users]\n",
        "    return ('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def create_query(query_dict):\n",
        "    tmp_string = ''\n",
        "    res = dict([(key, val) for key, val in \n",
        "           query_dict.items() if val])\n",
        "    del query_dict\n",
        "    query = ' '.join(res.values())\n",
        "    del res\n",
        "\n",
        "    return query\n",
        "\n",
        "\n",
        "  def crawler(self, query, error_counter=0):\n",
        "    # Creating list to append tweet data\n",
        "    tweets_list = []\n",
        "    try:\n",
        "      # Using TwitterUserScraper  TwitterSearchScraper to scrape data and append tweets to list\n",
        "      scraper = sntwitter.TwitterSearchScraper(query)\n",
        "      i = 0\n",
        "      for tweet in scraper.get_items(): #declare a username\n",
        "          if i >= self.max_results: #check number and date\n",
        "            break\n",
        "\n",
        "          tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.replyCount, tweet.retweetCount,\n",
        "                tweet.likeCount, tweet.user.username, tweet.lang, tweet.media, tweet.hashtags]) #declare the attributes to be returned\n",
        "          i += 1\n",
        "      \n",
        "    except Exception as e:\n",
        "      if 'Unable to find guest token' in str(e):\n",
        "          error_counter += 1\n",
        "          if error_counter > 3:\n",
        "            error_counter = 0\n",
        "            print(\"Sleep Time!\")\n",
        "            time.sleep(30.3 *60)\n",
        "            print(\"Morning!\")\n",
        "\n",
        "          return self.crawler(query, error_counter)\n",
        "\n",
        "      print(f\"query: {query} , {e}\")\n",
        "\n",
        "    # Creating a dataframe from the tweets list above \n",
        "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Reply Count',\n",
        "                                                    'Retweet Count', 'Like Count', 'Username', 'Lang', 'Media', 'Hashtags'])\n",
        "    return tweets_df\n",
        "\n",
        "\n",
        "\n",
        "  def user_crawler(self, user):\n",
        "    tmp_dict = self.query_dict.copy()\n",
        "    tmp_dict['from'] = (f'(from:{user})')\n",
        "    query = Twitter_scraper.create_query(tmp_dict)\n",
        "    del tmp_dict\n",
        "    return self.crawler(query)\n",
        "\n",
        "\n",
        "  def user_mode(self, user_list):   \n",
        "    user_crawler = self.user_crawler \n",
        "    pool = Pool(22)\n",
        "    df_list = pool.map(user_crawler, user_list)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    result_df = pd.concat(df_list, ignore_index=True)\n",
        "    return result_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from_users = ['CNN', 'FoxNews', 'ABC', 'BBCWorld', 'TIME', 'CBSNews', 'NBCNews', 'MSNBC','nytimes','washingtonpost']\n",
        "\n",
        "scraper = Twitter_scraper(max_results=10000, until=\"2020-01-01\", since=\"2019-01-01\",\n",
        "             lang=\"en\", from_users=from_users , exact_pharase=\"i'm good\")\n",
        "             \n",
        "tmp_query_dict = scraper.query_dict\n",
        "my_query = Twitter_scraper.create_query(tmp_query_dict)\n",
        "result = scraper.crawler(my_query)\n",
        "\n",
        "result.to_csv('test.csv', index=False)\n",
        "print(result.shape)\n",
        "\n",
        "\n",
        "path = 'test.csv'\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"python.py\", \"w\") as f:\n",
        "  f.write(prepared_code)\n",
        "\n"
      ],
      "metadata": {
        "id": "2LDAEJjaxNqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python python.py"
      ],
      "metadata": {
        "id": "glB1gSrrxNcV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}